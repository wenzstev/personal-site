<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <title>A First Stab at Training spaCy's Model</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&display=swap" rel="stylesheet">
  </head>
  <body>
    <div class="full-page">
    <div class="content">
    <header>
  <div class="blog-header">
      <h2>A First Stab at Training spaCy's Model</h2>
      <small>24 Jan 2020</small>
  </div>
</header>

<div class="margin">
<p>And so, with the evaluation of my heuristic model in my back pocket, it was time to bring out the big guns. I once again returned to the internet and consulted the NLP gods, and came back with a fairly straightfoward next step: it was time to try my hand at training spaCy’s NER model.</p>

<p>NER (Named Entity Recognition) is an aspect of Natural Language Programming that focuses on recognizing “stuff” in a body of text. This “stuff” can be whatever you want, such as names, locations, events… or in my case, recipe ingredients. Fortunately for me, spaCy comes with a fairly robust NER model, so I figured that my first order of business was to fire it up and run my test data through it, to see what I was working with.</p>

<p>This was easy enough:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">first_dataset</span> <span class="kn">import</span> <span class="n">ingredient_dataset</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'en_core_web_sm'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">ingredient_dataset</span><span class="p">:</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">ents</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">ent</span><span class="p">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">start_char</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">end_char</span><span class="p">,</span> <span class="n">ent</span><span class="p">.</span><span class="n">label_</span><span class="p">)</span></code></pre></figure>

<p>This example, like most of the examples in this post, are copied pretty directly from spaCy’s documentation, but I’m going to explain them all as I go as best I can. <code class="language-plaintext highlighter-rouge">doc.ents</code> is a list of all the named entites that spaCy’s NER model picked up, and this code simply cycles through all of them found and prints relevant information about them, namely the text, the locations of the first and last character, and the category of entity.</p>

<p>As would be expected from a list of 100 ingredient lines, this produced a lot of output, looking mostly like this:</p>

<p><img src="/assets/img/posts/ner_post/ner_working.png" alt="alt text" /></p>

<p>At this point, I could already see a few things that made me pretty excited. The model was already clever enough to recognize not only numbers, but collections of numbers, as you can see in the first and sixth lines in the screenshot. <code class="language-plaintext highlighter-rouge">'CARDINAL'</code> is spaCy’s category for numbers that are not associated with anything else.</p>

<p>The second thing that stood out was that spaCy recognized at least a few of the measurements as such, as could be seen on line 9 in the example. <code class="language-plaintext highlighter-rouge">'2 1/2 pounds'</code> is accurately read as a measurement, which is pretty cool. This was what got me thinking that maybe this wouldn’t be as difficult as I’d initially feared.</p>

<p>Finally, spaCy’s model read the word <code class="language-plaintext highlighter-rouge">'Garnish'</code> as <code class="language-plaintext highlighter-rouge">'NORP'</code>, which according to the documentation is a category for “Nationalities or religious or political groups.” This is obviously incorrect, but I found this example particularly fascinating because I suspect that spaCy was reading “-ish” as a suffix meaning “belonging to” (e.g., Engl<em>ish</em>, Span<em>ish</em>, Dan<em>ish</em>), and thought that the word “Garnish” was referring to an individual from the illustrious country of Garn. Dungeon Masters, take note: NLP is great for worldbuilding.</p>

<p>But fantasy countries notwithstanding, this was already a pretty strong showing, and it gave me a good idea of where to go next. Because it seemed easier to start by updating an already existing category than creating a new one from whole cloth (although I’ll be getting to that, don’t you worry), I decided that my next step was to update spaCy’s <code class="language-plaintext highlighter-rouge">'QUANTITY'</code> entity class to recognize cooking measurements. If I could get that to work, then I would move on to defining a new class for recipe ingredients.</p>

<p>The first thing I did was annotate some training data, using the recipe lines that I’d already collected. SpaCy’s model expects training data in a very specific style, but it was easy enough to find and copy the examples. Here’s the training set I used for my first try:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">TRAIN_DATA</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">'1 1/2 cups sugar'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'3/4 cup rye whiskey'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'3/4 cup brandy'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'1/2 cup rum'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'1 to 2 cups heavy cream, lightly whipped'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'2 1/2 pounds veal stew meat, cut into 2x1-inch pieces'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'4 tablespoons olive oil'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'1 1/2 cups chopped onion'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]}),</span>
    <span class="p">(</span><span class="s">'1 1/2 tablespoons chopped garlic'</span><span class="p">,</span> <span class="p">{</span><span class="s">"entities"</span><span class="p">:</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="s">"QUANTITY"</span><span class="p">)]})</span>
<span class="p">]</span></code></pre></figure>

<p>Breaking this down, we can see that the training data is stored as a list of tuples, where each tuple pairs a string with a collection of entities in the string. The entities are stored as a dictionary entry that is <em>itself</em> a list of tuples. In each of <em>those</em> tuples, there are three values: the start character of the entity, the end character of the entity, and the name of the entity category, as a string.</p>

<p>Confused yet? I was. I’m not entirely sure why the makers of spaCy decided on this particular format, but it’s flexible and can be pretty easily adapted to having numerous entities in a single string, or no entities.</p>

<p>And so, with my simple set of nine training examples, it was time to actually set up the training case. To do this, I again copied spaCy’s example model pretty much verbatim (which you can see <a href="https://spacy.io/usage/training#ner">here</a>), going through line by line to make sure I understood it all. And I’m pleased to say that I understood the majority of it, although to be honest there are a few concepts on display that stretch my working knowledge of Python. In particular, I plan to return and better familiarize myself with decorators(<code class="language-plaintext highlighter-rouge">@</code>), the use of the prefix asterisk (<code class="language-plaintext highlighter-rouge">*</code>), and the <code class="language-plaintext highlighter-rouge">plac</code> module. All three of those screamed “LEARN ME BETTER” when I googled their function. I put them on my ever-growing list of “things I need to learn how to do in Python,” and continued through the example.</p>

<p>There’s a lot of code in the example for me to just copy over into this post (especially since you can just read it in the link above), but I do want to call attention to the most important part. Basically, the model shuffles through the examples, and for each example, it gives its best guess of the entities in the example. It then compares its guess to the annotations to see if it was right. If it was wrong, it adjusts for the next time. This is done in this section:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">itn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">TRAIN_DATA</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># batch up the examples using spaCy's minibatch
</span>    <span class="n">batches</span> <span class="o">=</span> <span class="n">minibatch</span><span class="p">(</span><span class="n">TRAIN_DATA</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">compounding</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">1.001</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">texts</span><span class="p">,</span> <span class="n">annotations</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">nlp</span><span class="p">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">texts</span><span class="p">,</span>  <span class="c1"># batch of texts
</span>            <span class="n">annotations</span><span class="p">,</span>  <span class="c1"># batch of annotations
</span>            <span class="n">drop</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># dropout - make it harder to memorize data
</span>            <span class="n">losses</span><span class="o">=</span><span class="n">losses</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Losses: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span></code></pre></figure>

<p>That’s the most important part. Most of the rest is a framework to load, save, and test the model. There are some commands that need to be run to make sure the right part of spaCy’s model is trained, and the annotations need to be added into the NER. Overall, however, the process is pretty simple. Most of the heavy lifting is done behind the scenes.</p>

<p>I plugged all of the necessary information in, and after a few false starts… results.</p>

<p><img src="/assets/img/posts/ner_post/updated_quantity_ner.png" alt="alt_text" title="I believe my exact words here were, 'holy shit it's working'" /></p>

<p>IT… IS… ALIIIIIVE!</p>

<p>And how! Not only is it alive, but the resultant tests showed different results! As you can see from the screenshot above, the program was successfully registering entities such as <code class="language-plaintext highlighter-rouge">'1 1/2 cups'</code> and <code class="language-plaintext highlighter-rouge">'3/4 cup'</code> as <code class="language-plaintext highlighter-rouge">MEASUREMENTS</code>, just like we wanted! But the real test was yet to come. Showing a changed understanding of the <code class="language-plaintext highlighter-rouge">MEASUREMENT</code> entity was a promising start, but all it really showed was that the model had received the training data. In order to see if it had truly learned something new, I would have to turn my model out into the harsh wilds to see how it fared.</p>

<p>Fingers trembling, I loaded the entire set of 100 test ingredient lines into the machine, and…</p>

<p><img src="/assets/img/posts/ner_post/it_works_kind_of.png" alt="alt text" /></p>

<p>It works!</p>

<p>… kind of. As you can see it’s getting all of the measurements right but there are a ton of false positives now; possibly the model has been updated to think that anything with a number and a word after is in fact a quantity.</p>

<p>I’m guessing the way to fix this is to add some examples when that <em>isn’t</em> true. Additionally, I noticed that some of the previous entities that spaCy picked up aren’t being picked up anymore, which might be a problem. In the documentation, they warn of “catastrophic forgetting” as an issue, when teaching the model new things causes it to forget old ones. To fix this, spaCy recommends including some examples of other entities. To be honest, I’m not sure if this is as big of a problem as it would at first seem, considering the highly specialized nature of what I want this model to do.</p>

<p>But this is a hell of a start.</p>

</div>

    </div>
    <footer>
      <span><a href="/index.html">Home</a></span>
      <span><a href="/blog">Blog</a></span>
      <span>Contact me: <a href="mailto:wenzelstev@gmail.com">wenzelstev@gmail.com</a></span>
      <span><a href="https://www.linkedin.com/in/steven-wenzel/">LinkedIn</a></span>
      <span><a href="https://www.github.com/wenzstev">GitHub</a></span>
    </footer>
    </div>
  </body>
</html>
